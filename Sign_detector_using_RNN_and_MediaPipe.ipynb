{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "vkVUBFPcIQk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "SZDlqNyE1smI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T24qAcoyHyBT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KeyPoint using MP Holistic"
      ],
      "metadata": {
        "id": "pIiGCeuiJesR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic = mp.solutions.holistic  # Holisitic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
      ],
      "metadata": {
        "id": "1TNzZmkEYscP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image,model):\n",
        "  image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) # color conversion BGR 2 RGB\n",
        "  image.flags.writeable = False\n",
        "  results = model.process(image)\n",
        "  image.flags.writeable = True\n",
        "  image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)# color conversion RGB 2 BGR\n",
        "  return image,results"
      ],
      "metadata": {
        "id": "xmiup1d1ac6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "  mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
        "  mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
        "  mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
        "  mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # draw right hand connections"
      ],
      "metadata": {
        "id": "daCvcG0lfL5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic.FACEMESH_CONTOURS"
      ],
      "metadata": {
        "id": "NfUG0ovCwyk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw right hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             )"
      ],
      "metadata": {
        "id": "37sdePeeobNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "# access mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "  while cap.isOpened():\n",
        "\n",
        "    # Read feed\n",
        "    ret,frame = cap.read()\n",
        "    # make detections\n",
        "    image,results = mediapipe_detection(frame,holistic)\n",
        "    print(results)\n",
        "    # draw_landmarks(image,results)\n",
        "    draw_styled_landmarks(image, results)\n",
        "    # show to screen\n",
        "    cv2.imshow('OpenCV Feed',image)\n",
        "    # break gracefully\n",
        "    # if we press q then it will break\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "      break\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "GZxjrJllJkbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(results.face_landmarks.landmark)"
      ],
      "metadata": {
        "id": "MO9xRryNZO3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(results.left_hand_landmarks.landmark)"
      ],
      "metadata": {
        "id": "Sb4gu94cZQdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_styled_landmarks(image,results)"
      ],
      "metadata": {
        "id": "hLTVSpcUaBad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "DF5lpznKZZjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting key point Values"
      ],
      "metadata": {
        "id": "EgHakVFuZ0zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.pose_landmarks.landmark[0]"
      ],
      "metadata": {
        "id": "yKPSzKK0dVPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for res in results.pose_landmarks.landmark:\n",
        "  test = np.array([res.x,res.y,res.z,res.visibility])"
      ],
      "metadata": {
        "id": "nPRW2Nn6dYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "3XcdcgPOd805"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# till here we have one landmark to get all the land mark in a flattend array\n",
        "pose = []\n",
        "for res in results.pose_landmarks.landmark:\n",
        "  test = np.array([res.x,res.y,res.z,res.visibility])\n",
        "  pose.append(test)"
      ],
      "metadata": {
        "id": "FLpWXC-ad966"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#refarctoring the above code\n",
        "pose = np.array([[res.x,res.y,res.z,res.visibility]for res in results.pose_landmarks.landmark]).flatten()  if results.pose_landmarks else np.zeros(33*4)"
      ],
      "metadata": {
        "id": "qWbXOwRseYt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face = np.array([[res.x,res.y,res.z]for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)"
      ],
      "metadata": {
        "id": "K51i90NxhyI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lh = np.array([[res.x,res.y,res.z]for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)"
      ],
      "metadata": {
        "id": "7hyYWe_JeZ9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rh = np.array([[res.x,res.y,res.z]for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
      ],
      "metadata": {
        "id": "AuLKIcmmgwgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets make a fucntion for it to combine\n",
        "def extract_keypoints(results):\n",
        "  pose = np.array([[res.x,res.y,res.z,res.visibility]for res in results.pose_landmarks.landmark]).flatten()  if results.pose_landmarks else np.zeros(33*4)\n",
        "  face = np.array([[res.x,res.y,res.z]for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "  lh = np.array([[res.x,res.y,res.z]for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "  rh = np.array([[res.x,res.y,res.z]for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "  return np.concatenate([pose,face,lh,rh])"
      ],
      "metadata": {
        "id": "FUDMN0f0iniU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_keypoints(results).shape"
      ],
      "metadata": {
        "id": "w9dTN6T9ipEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup folders for Collection"
      ],
      "metadata": {
        "id": "WL9AZxJAkm-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for the exported data, numpy arrays\n",
        "DATA_PATH = os.path.join('MP_Data')\n",
        "\n",
        "#Actions that we try to detect\n",
        "actions = np.array(['hello','thanks','iloveyou'])\n",
        "\n",
        "# thirty videos worth of data\n",
        "no_sequences = 30\n",
        "\n",
        "# videos are going to be 30 frames in length\n",
        "sequence_length = 30\n"
      ],
      "metadata": {
        "id": "sJncb2BZk8mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hello\n",
        "## 0\n",
        "## 1\n",
        "## ..\n",
        "## ..\n",
        "## 29\n",
        "\n",
        "# thanks\n",
        "## 0\n",
        "\n",
        "# iloveyou\n",
        "##"
      ],
      "metadata": {
        "id": "MKKvY8olptST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for action in actions:\n",
        "  for sequence in range(no_sequences):\n",
        "    try:\n",
        "      os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))\n",
        "    except:\n",
        "      pass"
      ],
      "metadata": {
        "id": "UVRFakGLo7E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just to reccap , we're going to collect 30 videos per action i.e. hello , thanks, ily\n",
        "# then each one of the those video sequences are going to contain 30 frames of data.\n",
        "# Each frame will contain  1662 landmark values i.e. 3*30 sequences, 30 frames,1662 landmarks"
      ],
      "metadata": {
        "id": "8Hq08I3msMgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect keypoint Values for Training and Testing"
      ],
      "metadata": {
        "id": "6RqDYxQOs_Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "# access mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "  #loop through actions\n",
        "  for action in actions:\n",
        "    #loop through sequences aka videos\n",
        "    for sequence in range(no_sequences):\n",
        "      #loop through video length aka sequence_length\n",
        "      for frame_num in range(sequence_length):\n",
        "\n",
        "        # Read feed\n",
        "        ret,frame = cap.read()\n",
        "        # make detections\n",
        "        image,results = mediapipe_detection(frame,holistic)\n",
        "        print(results)\n",
        "        # draw_landmarks(image,results)\n",
        "        draw_styled_landmarks(image, results)\n",
        "\n",
        "        # Apply wait logic\n",
        "        if frame_num==0:\n",
        "          cv2.putText(image,'STARTING COLLECTION',(120,200),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),4,cv2.LINE_AA)\n",
        "          #passing image,name,position of name,font,fontsize,fontcolor,line width,line type\n",
        "          cv2.putText(image,'Collecting frames for {} video number {}'.format(action,sequence),(15,12),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
        "          cv2.waitKey(2000)\n",
        "        else:\n",
        "          cv2.putText(image,'Collecting frames for {} video number {}'.format(action,sequence),(15,12),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
        "\n",
        "        # New export keypoints\n",
        "        keypoints = extract_keypoints(results)\n",
        "        npy_path = os.path.join(DATA_PATH,action,str(sequence),str(frame_num))\n",
        "        np.save(npy_path,keypoints)\n",
        "        # show to screen\n",
        "        cv2.imshow('OpenCV Feed',image)\n",
        "        # break gracefully\n",
        "        # if we press q then it will break\n",
        "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "          break\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "4AylB2A0tG-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing Data and create labels and features"
      ],
      "metadata": {
        "id": "X45bpIKxLgxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "QFzdvCesJwW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "xmhmkCrXF-G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {label:num for num,label in enumerate(actions)}"
      ],
      "metadata": {
        "id": "uXPg_Y-zHDDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map"
      ],
      "metadata": {
        "id": "l-D7UETmLRxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences,labels =[],[]\n",
        "for action in actions:\n",
        "  for sequence in range(no_sequences):\n",
        "    window=[]\n",
        "    for frame_num in range(sequence_length):\n",
        "      res= np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n",
        "      window.append(res)\n",
        "    sequences.append(window)\n",
        "    labels.append(label_map[action])"
      ],
      "metadata": {
        "id": "eiUDAUQxLYDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sequences).shape"
      ],
      "metadata": {
        "id": "tXokbWuOM3vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(labels).shape"
      ],
      "metadata": {
        "id": "IWMf096BM6IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(sequences)"
      ],
      "metadata": {
        "id": "FgpGAmhKNFWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "-J8mLanwNN_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y= to_categorical(labels)"
      ],
      "metadata": {
        "id": "-1ItxXcTNO_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "EZcsaV22NT6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now converting the total data in to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.05)"
      ],
      "metadata": {
        "id": "RkiJvCC0Nb6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "ONSt7aUlNwHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Train LSTM Neural Network"
      ],
      "metadata": {
        "id": "jjU_q33uNyVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "metadata": {
        "id": "tmqlPfoDOPEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "import random\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# create our model\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64,return_sequences=True,activation=\"relu\",input_shape=(30,1662)),\n",
        "    LSTM(128,return_sequences=True,activation=\"relu\"),\n",
        "    LSTM(64,return_sequences=False,activation=\"relu\"),\n",
        "    Dense(64,activation=\"relu\"),\n",
        "    Dense(32,activation=\"relu\"),\n",
        "    Dense(actions.shape[0],activation=\"softmax\"),\n",
        "\n",
        "])\n"
      ],
      "metadata": {
        "id": "OZCxkqHpOdMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=[\"categorical_accuracy\"])"
      ],
      "metadata": {
        "id": "hqMm7WNaPoBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,y_train,epochs=150)"
      ],
      "metadata": {
        "id": "yaCHxbe8QESf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wget"
      ],
      "metadata": {
        "id": "aaysuaqzRXKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m wget https://raw.githubusercontent.com/code1ayush/DeepLearning_helper_functions/main/All_in_one_function.py"
      ],
      "metadata": {
        "id": "hz0KGVn_Qv3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation and training data separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "  \"\"\"\n",
        "  loss = history.history['loss']\n",
        "  # val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['categorical_accuracy']\n",
        "  # val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  # plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  # plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();"
      ],
      "metadata": {
        "id": "JV9H6s9zQxLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history)"
      ],
      "metadata": {
        "id": "7k-Aquo_R2tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Predictions"
      ],
      "metadata": {
        "id": "rxfnBiFOR-39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = model.predict(X_test)"
      ],
      "metadata": {
        "id": "8YpE_eybUBjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions[np.argmax(res[4])]"
      ],
      "metadata": {
        "id": "DGMSrTugUD5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions[np.argmax(y_test[4])]"
      ],
      "metadata": {
        "id": "6TMRrsjUULGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "HTQRNH_tUjIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('action.h5')"
      ],
      "metadata": {
        "id": "pIZep6xHUwIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = load_model('action.h5')"
      ],
      "metadata": {
        "id": "Gtyhneg0UyY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the confusion matrix"
      ],
      "metadata": {
        "id": "GK7zoGv9VVQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from All_in_one_function import make_confusion_matrix"
      ],
      "metadata": {
        "id": "UFv2iMAKYcYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "Bu5ar6_uZOFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.argmax(y_test,axis=1).tolist()\n",
        "y_pred = np.argmax(y_pred,axis=1).tolist()"
      ],
      "metadata": {
        "id": "sfmWO8cBZTBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(y_true,y_pred)"
      ],
      "metadata": {
        "id": "xCch97b3ZixB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test in real time"
      ],
      "metadata": {
        "id": "_CFJq1UeZoxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = []\n",
        "sentence = []\n",
        "threshold =0.4\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "# access mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "  while cap.isOpened():\n",
        "\n",
        "    # Read feed\n",
        "    ret,frame = cap.read()\n",
        "    # make detections\n",
        "    image,results = mediapipe_detection(frame,holistic)\n",
        "    print(results)\n",
        "    # draw_landmarks(image,results)\n",
        "    draw_styled_landmarks(image, results)\n",
        "\n",
        "    # predict logic\n",
        "    keypoints = extract_keypoints(results)\n",
        "\n",
        "    # sequence.append(keypoints)\n",
        "    sequence.insert(0,keypoints)\n",
        "    sequence = sequence[:30]\n",
        "\n",
        "    if len(sequence)==30:\n",
        "      res = model.predict(np.expand_dims(sequence,axis=0))[0]\n",
        "      print(actions[np.argmax(res)])\n",
        "\n",
        "    # video logic\n",
        "    if res[np.argmax(res)]>threshold:\n",
        "      sentence = actions[np.argmax(res)]\n",
        "      # if len(sentence)>0:\n",
        "      #   if actions[np.argmax(res)] != sentence[-1]:\n",
        "      #     sentence.append(actions[np.argmax(res)])\n",
        "      # else:\n",
        "      #     sentence.append(actions[np.argmax(res)])\n",
        "    # if len(sentence) >5:\n",
        "    #   sentence = sentence[-5:]\n",
        "\n",
        "    cv2.rectangle(image,(0,0),(110,40),(0,255,0),-1)\n",
        "    cv2.putText(image,sentence,(3,30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
        "    # show to screen\n",
        "    cv2.imshow('OpenCV Feed',image)\n",
        "    # break gracefully\n",
        "    # if we press q then it will break\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "      break\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "tMOIp09EZ44l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "cf8KHV1ldaje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[0].shape"
      ],
      "metadata": {
        "id": "G6D9w-jJa0ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but we want (1,30,1662) so\n",
        "np.expand_dims(X_test[0],axis=0).shape"
      ],
      "metadata": {
        "id": "Lc7CHhYoa4wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_fv7vKl_bCzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}